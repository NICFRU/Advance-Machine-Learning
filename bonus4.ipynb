{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NICFRU/Advance-Machine-Learning/blob/master/bonus4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3LkhKmUZ8Hu"
      },
      "source": [
        "## Tensflow "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLYB5ZqaZ8Hv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieDhJR7UZ8Hv"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laj-GmLPZ8Hw"
      },
      "outputs": [],
      "source": [
        "#tf.device('mps')\n",
        "tf.device('/cpu:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eKKqEEtZ8Hw"
      },
      "outputs": [],
      "source": [
        "!where tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRWB7USPZ8Hw"
      },
      "outputs": [],
      "source": [
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGuQJPayZ8Hw"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# cifar = tf.keras.datasets.cifar100\n",
        "# (x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
        "# model = tf.keras.applications.ResNet50(\n",
        "#     include_top=True,\n",
        "#     weights=None,\n",
        "#     input_shape=(32, 32, 3),\n",
        "#     classes=100,)\n",
        "\n",
        "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
        "# model.fit(x_train, y_train, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN-P9KL8Z8Hw"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "1KR0Pxz8bqsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnkqgZGVZ8Hw"
      },
      "outputs": [],
      "source": [
        "artists = pd.read_csv('/content/drive/MyDrive/Bonus 4/artists.csv')\n",
        "artists.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "sAcT41iFaGCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2fpuAW_Z8Hw"
      },
      "outputs": [],
      "source": [
        "artists.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKCt5mqeZ8Hw"
      },
      "outputs": [],
      "source": [
        "# Sort artists by number of paintings\n",
        "artists = artists.sort_values(by=['paintings'], ascending=False)\n",
        "\n",
        "# Create a dataframe with artists having more than 200 paintings\n",
        "artists_top = artists[artists['paintings'] >= 200].reset_index()\n",
        "artists_top = artists_top[['name', 'paintings']]\n",
        "#artists_top['class_weight'] = max(artists_top.paintings)/artists_top.paintings\n",
        "artists_top['class_weight'] = artists_top.paintings.sum() / (artists_top.shape[0] * artists_top.paintings)\n",
        "artists_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_AXlYNRZ8Hw"
      },
      "outputs": [],
      "source": [
        "images_dir = '/content/drive/MyDrive/Bonus 4/images'\n",
        "artists_dirs = os.listdir(images_dir)\n",
        "artists_top_name = artists_top['name'].str.replace(' ', '_').values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPhJTF-SZ8Hx"
      },
      "outputs": [],
      "source": [
        "# Augment data\n",
        "batch_size = 16\n",
        "train_input_shape = (224, 224, 3)\n",
        "n_classes = artists_top.shape[0]\n",
        "\n",
        "train_datagen = ImageDataGenerator(validation_split=0.2,\n",
        "                                   rescale=1./255.,\n",
        "                                   #rotation_range=45,\n",
        "                                   #width_shift_range=0.5,\n",
        "                                   #height_shift_range=0.5,\n",
        "                                   shear_range=5,\n",
        "                                   #zoom_range=0.7,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True,\n",
        "                                  )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory=images_dir,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=train_input_shape[0:2],\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    subset=\"training\",\n",
        "                                                    shuffle=True,\n",
        "                                                    classes=artists_top_name.tolist()\n",
        "                                                   )\n",
        "\n",
        "valid_generator = train_datagen.flow_from_directory(directory=images_dir,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=train_input_shape[0:2],\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    subset=\"validation\",\n",
        "                                                    shuffle=True,\n",
        "                                                    classes=artists_top_name.tolist()\n",
        "                                                   )\n",
        "\n",
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
        "print(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7QyprP4Z8Hx"
      },
      "outputs": [],
      "source": [
        "# Print a random paintings and it's random augmented version\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20,10))\n",
        "\n",
        "random_artist = random.choice(artists_top_name)\n",
        "random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n",
        "random_image_file = os.path.join(images_dir, random_artist, random_image)\n",
        "\n",
        "# Original image\n",
        "image = plt.imread(random_image_file)\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title(\"An original Image of \" + random_artist.replace('_', ' '))\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Transformed image\n",
        "aug_image = train_datagen.random_transform(image)\n",
        "axes[1].imshow(aug_image)\n",
        "axes[1].set_title(\"A transformed Image of \" + random_artist.replace('_', ' '))\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5Szf7QhZ8Hx"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-JUJsLyZ8Hx"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQV3ls9kZ8Hx"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTCp8YfbZ8Hx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG9OHJHwZ8Hx"
      },
      "outputs": [],
      "source": [
        "model_eff=EfficientNetB2(weights='imagenet', include_top=False, input_shape=train_input_shape\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kjWQ94xZ8Hx"
      },
      "outputs": [],
      "source": [
        "for layer in model_eff.layers:\n",
        "    layer.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jtNhkGzZ8Hx"
      },
      "outputs": [],
      "source": [
        "model_eff.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUjzoKa5Z8Hx"
      },
      "outputs": [],
      "source": [
        "# Add layers at the end\n",
        "X = base_model.output\n",
        "X = Flatten()(X)\n",
        "\n",
        "X = Dense(512, kernel_initializer='he_uniform')(X)\n",
        "#X = Dropout(0.5)(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "\n",
        "X = Dense(16, kernel_initializer='he_uniform')(X)\n",
        "#X = Dropout(0.5)(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "\n",
        "output = Dense(n_classes, activation='softmax')(X)\n",
        "\n",
        "model_res = Model(inputs=base_model.input, outputs=output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G4FXUX1Z8Hx"
      },
      "outputs": [],
      "source": [
        "X = model_eff.output\n",
        "X = Flatten()(X)\n",
        "\n",
        "X = Dense(512, kernel_initializer='he_uniform')(X)\n",
        "#X = Dropout(0.5)(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "\n",
        "X = Dense(16, kernel_initializer='he_uniform')(X)\n",
        "#X = Dropout(0.5)(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "\n",
        "output = Dense(n_classes, activation='softmax')(X)\n",
        "model_eff = Model(inputs=model_eff.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtOg72ZNZ8Hx"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lr=0.0001)\n",
        "model_res.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer, \n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eLGBuxrZ8Hx"
      },
      "outputs": [],
      "source": [
        "n_epoch = 10\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n",
        "                           mode='auto', restore_best_weights=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n",
        "                              verbose=1, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9lZ2WFqZ8Hy"
      },
      "outputs": [],
      "source": [
        "class_weights = artists_top['class_weight'].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMVCMAW-Z8Hy"
      },
      "outputs": [],
      "source": [
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naPfIXc6Z8Hy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6KBRj99Z8Hy"
      },
      "outputs": [],
      "source": [
        "# Train the model - all layers\n",
        "history1 = model_res.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n",
        "                              epochs=n_epoch,\n",
        "                              shuffle=True,\n",
        "                              verbose=1,\n",
        "                              callbacks=[reduce_lr],\n",
        "                              use_multiprocessing=True,\n",
        "                              workers=16,\n",
        "                              class_weight=class_weights\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yByEY0OvZ8Hy"
      },
      "outputs": [],
      "source": [
        "for layer in model_res.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in model_res.layers[:50]:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = Adam(lr=0.0001)\n",
        "\n",
        "model_res.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "n_epoch = 10\n",
        "history2 = model_res.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n",
        "                              epochs=n_epoch,\n",
        "                              shuffle=True,\n",
        "                              verbose=1,\n",
        "                              callbacks=[reduce_lr, early_stop],\n",
        "                              use_multiprocessing=True,\n",
        "                              workers=16,\n",
        "                              class_weight=class_weights\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEMXg0zJZ8Hy"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lr=0.0001)\n",
        "model_eff.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer, \n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOJ2dOdLZ8Hy"
      },
      "outputs": [],
      "source": [
        "history1_eff = model_eff.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n",
        "                              epochs=n_epoch,\n",
        "                              shuffle=True,\n",
        "                              verbose=1,\n",
        "                              callbacks=[reduce_lr],\n",
        "                              use_multiprocessing=True,\n",
        "                              workers=16,\n",
        "                              class_weight=class_weights\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CDmQfv1Z8Hy"
      },
      "outputs": [],
      "source": [
        "for layer in model_eff.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in model_eff.layers[:50]:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = Adam(lr=0.0001)\n",
        "\n",
        "model_eff.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "n_epoch = 10\n",
        "history2_eff = model_eff.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n",
        "                              epochs=n_epoch,\n",
        "                              shuffle=True,\n",
        "                              verbose=1,\n",
        "                              callbacks=[reduce_lr, early_stop],\n",
        "                              use_multiprocessing=True,\n",
        "                              workers=16,\n",
        "                              class_weight=class_weights\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gih4q4ScZ8Hy"
      },
      "outputs": [],
      "source": [
        "# Merge history1 and history2\n",
        "history = {}\n",
        "history['loss'] = history1.history['loss'] + history2.history['loss']\n",
        "history['acc'] = history1.history['accuracy'] + history2.history['accuracy']\n",
        "history['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\n",
        "history['val_acc'] = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
        "history['lr'] = history1.history['lr'] + history2.history['lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFznWe-tZ8Hy"
      },
      "outputs": [],
      "source": [
        "# Merge history1 and history2\n",
        "history_eff = {}\n",
        "history_eff['loss'] = history1_eff.history['loss'] + history2_eff.history['loss']\n",
        "history_eff['acc'] = history1_eff.history['accuracy'] + history2_eff.history['accuracy']\n",
        "history_eff['val_loss'] = history1_eff.history['val_loss'] + history2_eff.history['val_loss']\n",
        "history_eff['val_acc'] = history1_eff.history['val_accuracy'] + history2_eff.history['val_accuracy']\n",
        "history_eff['lr'] = history1_eff.history['lr'] + history2_eff.history['lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h4qeewPZ8Hy"
      },
      "outputs": [],
      "source": [
        "def plot_training(history):\n",
        "    acc = history['acc']\n",
        "    val_acc = history['val_acc']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
        "    \n",
        "    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n",
        "    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n",
        "    axes[0].set_title('Training and Validation Accuracy')\n",
        "    axes[0].legend(loc='best')\n",
        "\n",
        "    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n",
        "    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n",
        "    axes[1].set_title('Training and Validation Loss')\n",
        "    axes[1].legend(loc='best')\n",
        "    \n",
        "    plt.show()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTUJuvPxZ8Hy"
      },
      "outputs": [],
      "source": [
        "def plot_training(history):\n",
        "    acc = history['acc']\n",
        "    val_acc = history['val_acc']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
        "    \n",
        "    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n",
        "    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n",
        "    axes[0].set_title('Training and Validation Accuracy')\n",
        "    axes[0].legend(loc='best')\n",
        "\n",
        "    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n",
        "    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n",
        "    axes[1].set_title('Training and Validation Loss')\n",
        "    axes[1].legend(loc='best')\n",
        "    \n",
        "    plt.show()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIWLQzQcZ8Hz"
      },
      "outputs": [],
      "source": [
        "plot_training(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JadxbCexZ8Hz"
      },
      "outputs": [],
      "source": [
        "plot_training(history_eff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpNOECoyZ8Hz"
      },
      "outputs": [],
      "source": [
        "# Prediction accuracy on train data\n",
        "score = model_res.evaluate_generator(train_generator, verbose=1)\n",
        "print(\"Prediction accuracy on train data =\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNLiSPNXZ8Hz"
      },
      "outputs": [],
      "source": [
        "score_eff = model_eff.evaluate_generator(train_generator, verbose=1)\n",
        "print(\"Prediction accuracy on train data =\", score_eff[1])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.15 ('Bonu_Aufgaben')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cc1f5513a3e8f4f63cdc61ef27abe172a38277dcac41ce1532556897ed7fb831"
      }
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}